
## ArrayBuffer
ArrayBuffer是内存中的一块二进制缓冲区，只分配内存空间，不能直接读取或修改 ArrayBuffer 的内容，必须通过「视图（View）」（如 Uint8Array、DataView）来操作二进制数据。

占用内存极低，是数据实际字节数（无额外开销）；
* 对比字符串：JS 中字符串是 UTF-16 编码，1 个字符占用 2 字节，30M 二进制数据转字符串后会占用 60M 内存，而 ArrayBuffer 仅需 30M；
* 对比 Blob：Blob 是 “二进制数据的包装对象”（含元数据），

```js
// 创建一个 8 字节的 ArrayBuffer（内存中开辟 8 个连续字节空间）
const buffer = new ArrayBuffer(8); 
console.log(buffer.byteLength); // 8（固定大小）

// 通过 Uint8Array 视图操作缓冲区（按“无符号8位整数”解读）
const view = new Uint8Array(buffer);
view[0] = 255; // 给第1个字节赋值 255（二进制：11111111）
view[1] = 0;   // 给第2个字节赋值 0（二进制：00000000）
console.log(view); // Uint8Array(8) [255, 0, 0, 0, 0, 0, 0, 0]
```

## SparkMD5 增量哈希
SparkMD5 哈希后的结果是 32 位小写 16 进制字符串（默认），是 MD5 算法的标准输出，具有唯一性、统一性、不可逆向性

```js
new SparkMD5();  // 字符串模式，适合小文本、短字符串，性能极差（会爆栈）
new SparkMD5.Blob();   // Blob模式，适合：小文件（<100M），无需 FileReader
new SparkMD5.ArrayBuffer()  // 大文件分片、二进制数据哈希。（内存 + 性能双佳）
```

| 模式| 	接收的数据类型	| 核心用途	| 大文件场景适配性| 
| ---- |---- |---- |---- |
| SparkMD5()（默认）|	字符串（String）|	小文本、短字符串哈希|	❌ 极差（内存爆炸）| 
| SparkMD5.Blob()|	Blob/File 对象|	直接处理 Blob/File（无需 FileReader）|	✅ 可用，但灵活性略低| 
| SparkMD5.ArrayBuffer()（推荐）|	ArrayBuffer（二进制缓冲区）	大文件分片、二进制数据哈希	|✅ 最优（内存 + 性能双佳）| 


### 大文件全量哈希
```js
// 错误示例：300G文件直接读入内存，瞬间OOM
const spark = new SparkMD5.ArrayBuffer();
const reader = new FileReader();

reader.readAsArrayBuffer(largeFile); // 尝试加载300G数据到内存
reader.onload = (e) => {
  spark.append(e.target.result); // 一次性传入全量数据（不可能成功）
  const md5 = spark.end();
};
```

### 大文件增量哈希
把大文件（或大数据）用slice分成多块（比如 30M/片），用FileReader读取出来后 逐块把数据 “喂给” SparkMD5，它会**累积每一块的哈希状态**，最后合并出整个数据的最终 MD5，而不用把所有数据一次性加载到内存。

```js
// 正确示例：分块传入，内存仅30M
const spark = new SparkMD5.ArrayBuffer();
let offset = 0;
const CHUNK_SIZE = 30 * 1024 * 1024;

function readNextChunk() {
  const chunk = largeFile.slice(offset, offset + CHUNK_SIZE); // 分块（30M）
  const reader = new FileReader();
  reader.readAsArrayBuffer(chunk); // 仅加载当前块到内存
  reader.onload = (e) => {
    spark.append(e.target.result); // 增量：把当前块的状态累加上去
    offset += CHUNK_SIZE;
    if (offset < largeFile.size) {
      readNextChunk(); // 继续处理下一块
    } else {
      const md5 = spark.end(); // 所有块处理完，生成最终MD5
    }
  };
}
readNextChunk();
```


